# ğŸš€ Spark Integration - Complete Installation Guide

## âœ… All Files Created Successfully!

### ğŸ“¦ What Was Added (8 New Files)

1. **Configuration**
   - âœ… `app/config/spark_config.py` - Spark session management

2. **Services**
   - âœ… `app/services/spark_base.py` - Base utilities
   - âœ… `app/services/spark_parquet_service.py` - Parquet operations
   - âœ… `app/services/spark_delta_service.py` - Delta Lake operations

3. **API**
   - âœ… `app/api/spark_routes.py` - REST endpoints

4. **Documentation**
   - âœ… `SPARK_GUIDE.md` - Complete tutorial
   - âœ… `SPARK_SETUP.md` - Quick start
   - âœ… `SPARK_SUMMARY.md` - Overview

5. **Testing**
   - âœ… `scripts/test_spark_local.py` - Test suite

6. **Dependencies**
   - âœ… Updated `requirements.txt` with PySpark

---

## ğŸ¯ Installation Steps

### Step 1: Install Java (Required for Spark)

**Check if Java is installed:**
```powershell
java -version
```

**If not installed, download Java 11 or 17 (LTS):**
- Visit: https://adoptium.net/
- Download **Temurin 17 (LTS)** for Windows
- Install and restart PowerShell

### Step 2: Install Python Dependencies

```powershell
# Make sure you're in the backend directory
cd C:\Users\ashis\Desktop\metadata\backend

# Activate virtual environment
.\venv\Scripts\Activate.ps1

# Install new dependencies
pip install -r requirements.txt
```

**This will install:**
- `pyspark==3.5.0` - Apache Spark
- `delta-spark==3.0.0` - Delta Lake
- And all related dependencies

### Step 3: Verify Installation

```powershell
# Run the test suite
cd scripts
python test_spark_local.py
```

**Expected Output:**
```
ğŸŒŸğŸŒŸğŸŒŸ SPARK INTEGRATION TEST SUITE ğŸŒŸğŸŒŸğŸŒŸ

TEST 1: Spark Session Creation
âœ… Spark session created successfully
   Spark version: 3.5.0
   Master: local[*]
   Web UI: http://localhost:4040

TEST 2: DataFrame Creation and Operations
âœ… DataFrame created
...

TEST SUMMARY
âœ… PASS - Spark Session
âœ… PASS - DataFrame Operations
âœ… PASS - Parquet Operations
âœ… PASS - SQL Operations
âœ… PASS - Performance

Total: 5/5 tests passed

ğŸ‰ All tests passed! Spark is working correctly.
```

### Step 4: Start the API

```powershell
# Go back to backend directory
cd ..

# Start the FastAPI server
python -m app.main
```

**You should see:**
```
ğŸš€ Starting Metastore Viewer API
ğŸ“š API Documentation available at:
   Swagger UI: http://0.0.0.0:8000/docs
```

### Step 5: Test API Endpoints

**Open browser and visit:**
- **Swagger UI:** http://localhost:8000/docs
- **Spark UI:** http://localhost:4040 (when Spark is running)

**Look for new endpoints under "Spark Processing" tag:**
- `/api/spark/parquet/query`
- `/api/spark/parquet/modify`
- `/api/spark/delta/read`
- `/api/spark/delta/update`
- ... and more!

---

## ğŸ“– Next Steps - Learning Path

### Day 1: Understanding (1-2 hours)

1. **Read Quick Start (10 min)**
   ```powershell
   # Open in VS Code or browser
   code SPARK_SETUP.md
   ```

2. **Read Complete Guide (30 min)**
   ```powershell
   code SPARK_GUIDE.md
   ```

3. **Read Code Comments (30 min)**
   - Open `app/config/spark_config.py`
   - Read through the comments
   - Every line is explained!

### Day 2: Practice (2-3 hours)

1. **Create Test Data**
   ```powershell
   cd scripts
   python create_format_examples.py
   ```
   This creates sample Parquet and Delta files.

2. **Upload to S3**
   ```powershell
   aws s3 sync sample_data/parquet/ s3://YOUR-BUCKET/test-data/parquet/
   aws s3 sync sample_data/delta/ s3://YOUR-BUCKET/test-data/delta/
   ```

3. **Try API Endpoints**
   - Visit http://localhost:8000/docs
   - Try "POST /api/spark/parquet/query"
   - Use your S3 credentials and test data

### Day 3: Advanced (2-3 hours)

1. **Test with Real Data**
   - Use your actual S3 data
   - Compare Spark vs DuckDB performance
   - Monitor in Spark UI (http://localhost:4040)

2. **Try Delta Lake Features**
   - ACID updates
   - Time travel
   - Merge (upsert) operations

3. **Optimize Performance**
   - Adjust memory settings
   - Test partitioning strategies
   - Benchmark different file sizes

---

## ğŸ“ Key Concepts to Understand

### 1. **Lazy Evaluation**
```python
# These are ALL lazy - no computation yet
df = spark.read.parquet("file.parquet")
df = df.filter("age > 25")
df = df.select("name", "salary")

# THIS triggers execution (action)
df.show()
```

### 2. **Partitions**
```python
# Data is split into chunks
# Each chunk processes in parallel
# More partitions = more parallelism
df.rdd.getNumPartitions()  # Check how many
```

### 3. **DataFrame Pipeline**
```
READ â†’ TRANSFORM â†’ TRANSFORM â†’ ACTION
 â†“         â†“           â†“          â†“
Lazy     Lazy       Lazy      Executes
```

### 4. **Spark vs DuckDB**
- **Small files (< 100 MB)** â†’ DuckDB is faster
- **Large files (> 100 MB)** â†’ Spark is better
- **Need ACID updates** â†’ Use Spark + Delta Lake

---

## ğŸ” Monitoring & Debugging

### Spark Web UI (http://localhost:4040)
Shows:
- âœ… Jobs and stages
- âœ… Task execution times
- âœ… Memory usage
- âœ… SQL query plans
- âœ… Storage (cached data)

**Always check this when Spark is slow!**

### Application Logs
```python
# Logs show what's happening
import logging
logger = logging.getLogger(__name__)

# You'll see messages like:
# "ğŸ“Š Reading Delta table: s3://bucket/table"
# "âœ… Query completed: 1,000 rows"
```

---

## ğŸ› ï¸ Troubleshooting Guide

### Problem: "Java not found"
```powershell
# Install Java 11 or 17
# From: https://adoptium.net/
# Then restart PowerShell
```

### Problem: "Out of memory"
**Solution:** Edit `app/config/spark_config.py`
```python
# Increase memory
conf.set("spark.driver.memory", "8g")  # Was 4g
```

### Problem: Tests fail
```powershell
# Check Java
java -version  # Should be 8, 11, or 17

# Reinstall dependencies
pip install --force-reinstall pyspark delta-spark

# Run tests again
python scripts/test_spark_local.py
```

### Problem: Spark UI not accessible
- Spark UI only appears when Spark is running
- Start API or run tests first
- Then visit http://localhost:4040

### Problem: Slow queries
1. Check partitions: `df.rdd.getNumPartitions()`
2. Repartition: `df = df.repartition(8)`
3. Cache if reusing: `df.cache()`
4. Monitor in Spark UI

---

## ğŸ“Š Quick Reference

### File Structure
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ spark_config.py        # Spark configuration
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ spark_base.py         # Base utilities
â”‚   â”‚   â”œâ”€â”€ spark_parquet_service.py
â”‚   â”‚   â””â”€â”€ spark_delta_service.py
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ spark_routes.py        # REST endpoints
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ test_spark_local.py       # Test suite
â”œâ”€â”€ SPARK_GUIDE.md                # Complete tutorial
â”œâ”€â”€ SPARK_SETUP.md                # Quick start
â””â”€â”€ SPARK_SUMMARY.md              # Overview
```

### Important Commands
```powershell
# Test Spark
python scripts\test_spark_local.py

# Start API
python -m app.main

# Create sample data
python scripts\create_format_examples.py

# Check Spark UI
# Open: http://localhost:4040
```

### API Endpoints
```
GET  /api/spark/health           # Check status
POST /api/spark/parquet/query    # Query Parquet
POST /api/spark/parquet/modify   # Modify Parquet
POST /api/spark/delta/read       # Read Delta
POST /api/spark/delta/update     # Update Delta
POST /api/spark/delta/delete     # Delete from Delta
POST /api/spark/delta/merge      # Upsert to Delta
```

---

## ğŸ‰ Success Checklist

- [ ] Java installed and verified
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Tests pass (5/5)
- [ ] API starts successfully
- [ ] Spark UI accessible (http://localhost:4040)
- [ ] API docs show Spark endpoints
- [ ] Read SPARK_SETUP.md
- [ ] Read SPARK_GUIDE.md
- [ ] Understand code comments

---

## ğŸ’¡ Pro Tips

1. **Always check Spark UI** when debugging
2. **Filter early** in your pipeline
3. **Select only needed columns** (column pruning)
4. **Cache wisely** - only data used multiple times
5. **Partition by commonly filtered columns**
6. **Use Delta Lake for updates** - ACID is worth it

---

## ğŸš€ You're Ready!

**What you have now:**
- âœ… Production-ready Spark integration
- âœ… Local execution (FREE)
- âœ… ACID transactions with Delta Lake
- âœ… 10 new API endpoints
- âœ… Comprehensive documentation
- âœ… Clean, educational code
- âœ… Full test suite

**Start with:**
1. Run `python scripts\test_spark_local.py`
2. Open http://localhost:8000/docs
3. Try the Spark endpoints
4. Monitor at http://localhost:4040

**Questions? Check:**
- [SPARK_SETUP.md](SPARK_SETUP.md) - Quick answers
- [SPARK_GUIDE.md](SPARK_GUIDE.md) - Detailed explanations
- Code comments - Every line explained

---

**Congratulations! You now have Apache Spark integrated into your project! ğŸŠ**

**Happy Sparking! âš¡**
